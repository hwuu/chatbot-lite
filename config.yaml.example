# LLM 配置
llm:
  api_base: "http://localhost:11434/v1"  # Ollama 本地地址
  model: "qwen2.5-coder:7b"              # 模型名称
  api_key: "ollama"                      # 本地模型通常不验证，保留字段
  temperature: 0.7                       # 生成温度
  max_tokens: 2000                       # 最大 token 数
  system_prompt: |
    你是一个问答助手，回答要简明扼要，使用中文，输出使用 markdown 格式。
    如果有什么不确定的问题，要反问用户，不要自己猜。

# 应用配置
app:
  history_dir: "~/.clichat"              # 对话历史存储目录
  context_strategy: "lazy_compress"      # 上下文管理策略
  compress_threshold: 0.85               # Token 达到 85% 时触发压缩
  compress_summary_tokens: 300           # 压缩后摘要的目标长度
  markdown_code_theme: "github-dark"     # Markdown 代码块主题（可选: github-dark, dracula, one-dark, solarized-dark, nord 等）
  ui_theme: "textual-dark"               # UI 主题（可选: textual-dark, nord, gruvbox, monokai, dracula, tokyo-night 等）
